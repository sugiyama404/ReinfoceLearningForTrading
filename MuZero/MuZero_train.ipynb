{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuZero_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNEuc3xxwoFpnfbX0ptgDHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/MuZero/MuZero_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gOu66rFVeAVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5539b1-3cf2-421c-f0ab-d7ca711b9eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "from google.colab import drive\n",
        "import copy\n",
        "import math\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from statistics import mean\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import collections\n",
        "import typing\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "from tensorflow.keras.models import Sequential, clone_model, Model\n",
        "from tensorflow.keras.layers import (Dense, ReLU, Input, Lambda, LSTM, Activation,\n",
        "                                     GlobalAveragePooling1D, Flatten,\n",
        "                                     MaxPool1D, Conv1D, Add, BatchNormalization, AveragePooling1D)\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import categorical_crossentropy, mean_squared_error\n",
        "from tensorflow.keras.utils import Progbar\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from multiprocessing import Process\n",
        "import multiprocessing\n",
        "\n",
        "import pprint\n",
        "\n",
        "mode = 'train'\n",
        "name = 'muzero'\n",
        "level = 1\n",
        "if level == 2:\n",
        "    name += name + 'lv2'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir  = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir  = 'Colab Notebooks/workspace/export/'\n",
        "game_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### Helpers ##########\n",
        "MAXIMUM_FLOAT_VALUE = float('inf')\n",
        "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
        "\n",
        "# ツリーの最小値を保持するクラス。\n",
        "class MinMaxStats:\n",
        "    def __init__(self, known_bounds: Optional[KnownBounds]):\n",
        "        self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        if self.maximum > self.minimum:\n",
        "        # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value"
      ],
      "metadata": {
        "id": "JpVLJCyn5Fnr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MuZeroConfig:\n",
        "    def __init__(self,\n",
        "                action_space_size: int,\n",
        "                max_moves: int,\n",
        "                discount: float,\n",
        "                dirichlet_alpha: float,\n",
        "                num_simulations: int,\n",
        "                batch_size: int,\n",
        "                td_steps: int,\n",
        "                num_actors: int,\n",
        "                lr_init: float,\n",
        "                lr_decay_steps: float,\n",
        "                visit_softmax_temperature_fn,\n",
        "                known_bounds: Optional[KnownBounds] = None):\n",
        "        ### Self-Play\n",
        "        self.action_space_size = action_space_size\n",
        "        self.num_actors = num_actors\n",
        "\n",
        "        self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
        "        self.max_moves = max_moves\n",
        "        self.num_simulations = num_simulations\n",
        "        self.discount = discount\n",
        "\n",
        "        # ルート事前探査ノイズ。\n",
        "        self.root_dirichlet_alpha = dirichlet_alpha\n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB式\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "        # 環境で発生する値に関する情報がすでにある場合は、\n",
        "        # それらを使用して再スケーリングを初期化できます。\n",
        "        # これは厳密には必要ありませんが、ボードゲームでAlphaZeroと同じ動作を確立します。\n",
        "        self.known_bounds = known_bounds\n",
        "\n",
        "        ### Training\n",
        "        self.training_steps = int(1000)\n",
        "        self.checkpoint_interval = int(2)\n",
        "        self.window_size = int(100)\n",
        "        self.batch_size = batch_size\n",
        "        self.num_unroll_steps = 5\n",
        "        self.td_steps = td_steps\n",
        "\n",
        "        self.weight_decay = 1e-4\n",
        "        self.momentum = 0.9\n",
        "\n",
        "        # 指数学習率のスケジュール\n",
        "        self.lr_init = lr_init\n",
        "        self.lr_decay_rate = 0.1\n",
        "        self.lr_decay_steps = lr_decay_steps\n",
        "\n",
        "        self.env = Environment(df, initial_money=1000000, mode='train')\n",
        "        self.scaler, self.scaler2 = self._standard_scaler(self.env)\n",
        "        \n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        rewards = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            rewards.append([reward])\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler2 = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        scaler2.fit(rewards)\n",
        "        return scaler, scaler2\n",
        "\n",
        "    def new_game(self):\n",
        "        return Game(self.action_space_size, self.discount,\n",
        "                    self.env, self.scaler, self.scaler2)\n",
        "\n",
        "def make_trade_config() -> MuZeroConfig:\n",
        "    def visit_softmax_temperature(num_moves, training_steps):\n",
        "        if training_steps < 500e3:\n",
        "            return 1.0\n",
        "        elif training_steps < 600e3:\n",
        "            return 0.75\n",
        "        elif training_steps < 750e3:\n",
        "            return 0.5\n",
        "        else:\n",
        "            return 0.25\n",
        "\n",
        "    return MuZeroConfig(\n",
        "        action_space_size=3,\n",
        "        max_moves=10000,  # Half an hour at action repeat 4.\n",
        "        discount=0.997,\n",
        "        dirichlet_alpha=0.25,\n",
        "        num_simulations=50,\n",
        "        batch_size=1024,\n",
        "        td_steps=10,\n",
        "        num_actors=4,\n",
        "        lr_init=0.05,\n",
        "        lr_decay_steps=350e3,\n",
        "        visit_softmax_temperature_fn=visit_softmax_temperature)"
      ],
      "metadata": {
        "id": "OnAPNXOW5FVF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Action:\n",
        "    '''\n",
        "    a = Action(0)\n",
        "    b = Action(2)\n",
        "\n",
        "    mydict = {a: \"value for 0\", b: \"value for 2\"}\n",
        "    print(mydict[a], mydict[b]) # value for 0 value for 2\n",
        "    a.index = 2                     # →ハッシュ値が変わる\n",
        "    print(mydict[a], mydict[b]) # value for 2 value for 2\n",
        "    c = Action(3)\n",
        "    print(a in mydict)\n",
        "    print(c > a)\n",
        "    print(c == a)\n",
        "    '''\n",
        "    def __init__(self, index: int):\n",
        "        '''コンストラクタ'''\n",
        "        self.index = index\n",
        "\n",
        "    def __hash__(self):\n",
        "        '''hash呼び出し時に呼び出される'''\n",
        "        return self.index\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        '''同値の時に呼び出される'''\n",
        "        return self.index == other.index\n",
        "\n",
        "    def __gt__(self, other):\n",
        "        '''大小比較された時に呼び出される'''\n",
        "        return self.index > other.index"
      ],
      "metadata": {
        "id": "JcflUsnuJe3l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self) -> bool:\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self) -> float:\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count"
      ],
      "metadata": {
        "id": "itiiZjCYTeud"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionHistory:\n",
        "    '''検索内で使用されるシンプルな履歴コンテナ。\n",
        "    実行されたアクションを追跡するためにのみ使用されます。'''\n",
        "    def __init__(self, history: List[Action], action_space_size: int):\n",
        "        self.history = list(history)\n",
        "        self.action_space_size = action_space_size\n",
        "\n",
        "    def clone(self):\n",
        "        return ActionHistory(self.history, self.action_space_size)\n",
        "\n",
        "    def add_action(self, action: Action):\n",
        "        self.history.append(action)\n",
        "\n",
        "    def last_action(self) -> Action:\n",
        "        return self.history[-1]\n",
        "\n",
        "    def action_space(self) -> List[Action]:\n",
        "        return [Action(i) for i in range(self.action_space_size)]"
      ],
      "metadata": {
        "id": "Ws2lFQnGKHuQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test', commission = 0):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.commission      = commission\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "        self.sell_price      = None\n",
        "        self.buy_price       = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "        self.sell_price      = 0\n",
        "        self.buy_price       = 0\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self.sell_price = 0\n",
        "        self._trade(action,done)\n",
        "        reward = 0\n",
        "        if (self.sell_price > 0) and (self.buy_price > 0) and ((self.sell_price - self.buy_price) != 0):\n",
        "            reward = (self.sell_price - self.buy_price) / self.buy_price\n",
        "            self.buy_price = 0\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            if self.hold_a_position != 0:\n",
        "                self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                self.hold_a_position = 0\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.buy_price += self.now_price\n",
        "                            self.cash_in_hand -= self.now_price + self.commission * self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.sell_price += self.now_price * self.hold_a_position\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position - self.commission * self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    self.trade_time += 1\n",
        "                    if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                        self.trade_win += 1"
      ],
      "metadata": {
        "id": "UesmX6O1olab"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 環境との相互作用の単一のエピソード。\n",
        "class Game:\n",
        "    def __init__(self, action_space_size: int,\n",
        "                 discount: float, env: Environment,\n",
        "                 scaler: StandardScaler, scaler2: StandardScaler):\n",
        "        self.env = env  # Game specific environment.\n",
        "        self.history = []\n",
        "        self.rewards = []\n",
        "        self.image = [] # (長さ, 32, 6) 出力用\n",
        "        self.image_histroy = [] # (長さ, 6) 一時保存保存用\n",
        "        self.child_visits = []\n",
        "        self.root_values = []\n",
        "        self.action_space_size = action_space_size\n",
        "        self.discount = discount\n",
        "        self.terminal_flag = False\n",
        "        self.info = { 'cur_revenue' : 1.0 , 'trade_time' : 1.0, 'trade_win' : 1.0 }\n",
        "\n",
        "        self.scaler = scaler\n",
        "        self.scaler2 = scaler2\n",
        "\n",
        "        self.act_onehot = [[ 0, 0, 0],  # [0]: buy\n",
        "                           [ 0, 1, 0],  # [1]: hold\n",
        "                           [ 0, 0, 1]]  # [2]: sell\n",
        "        _ = self.env.reset()\n",
        "\n",
        "    # ゲーム固有の終了ルール。\n",
        "    def terminal(self) -> bool:\n",
        "        return self.terminal_flag\n",
        "\n",
        "    # 法的措置のゲーム固有の計算。\n",
        "    def legal_actions(self) -> List[Action]:\n",
        "        act_list = [Action(0), Action(1)]\n",
        "        if self.env.hold_a_position != 0:\n",
        "            act_list = [Action(1), Action(2)]\n",
        "        return act_list\n",
        "\n",
        "    # 環境を進める。\n",
        "    def apply(self, action: Action):\n",
        "        act = action.index\n",
        "        state, reward, done, info = self.env.step(act)\n",
        "        if done:\n",
        "            self.terminal_flag = done\n",
        "            self.info = info\n",
        "\n",
        "        if len(self.image_histroy) >= 31:\n",
        "            reward = self.scaler2.transform([[reward]])\n",
        "            reward = reward[0][0]\n",
        "            self.rewards.append(reward)\n",
        "            self.history.append(action)\n",
        "        self._make_image_histroy(state, act)\n",
        "\n",
        "    # image_histroyとimageの生成\n",
        "    def _make_image_histroy(self, state: list, act: int):\n",
        "        state = self.scaler.transform([state])\n",
        "        image = state[0].tolist() + self.act_onehot[act]\n",
        "        self.image_histroy.append(image)\n",
        "        if len(self.image_histroy) >= 32:\n",
        "            self.image.append(self.image_histroy[-32:])\n",
        "\n",
        "    def store_search_statistics(self, root: Node):\n",
        "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "        action_space = (Action(index) for index in range(self.action_space_size))\n",
        "        self.child_visits.append([\n",
        "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "            for a in action_space\n",
        "        ])\n",
        "        self.root_values.append(root.value())\n",
        "\n",
        "    # ゲーム固有の特徴平面(obs)\n",
        "    def make_image(self, state_index: int):\n",
        "        return [self.image[state_index]]\n",
        "\n",
        "    # 値ターゲットは、検索ツリーNステップの割引ルート値と、それまでのすべての報酬の割引合計です。\n",
        "    def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int):\n",
        "        '''\n",
        "        input: state_index: int, num_unroll_steps: 5, td_steps: 10)\n",
        "        output: target_value: TD目標価値(z), target_reward: 即時報酬(u), target_policy: MCTSポリシー(pai)\n",
        "        '''\n",
        "        targets = []\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "            bootstrap_index = current_index + td_steps\n",
        "            if bootstrap_index < len(self.root_values):\n",
        "                value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
        "                value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
        "\n",
        "            if current_index < len(self.root_values):\n",
        "                targets.append((value, self.rewards[current_index],\n",
        "                                self.child_visits[current_index]))\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states.\n",
        "                targets.append((0, 0, [0.333, 0.334, 0.333]))\n",
        "        return targets\n",
        "\n",
        "    def action_history(self) -> ActionHistory:\n",
        "        return ActionHistory(self.history, self.action_space_size)"
      ],
      "metadata": {
        "id": "wwV-6duMeHqL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, config: MuZeroConfig):\n",
        "        self.window_size = config.window_size\n",
        "        self.batch_size = config.batch_size\n",
        "        self.buffer = []\n",
        "\n",
        "    def save_game(self, game):\n",
        "        if len(self.buffer) > self.window_size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(game)\n",
        "\n",
        "    def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
        "        games = [self.sample_game() for _ in range(self.batch_size)]\n",
        "        game_pos = [(g, self.sample_position(g, num_unroll_steps, td_steps))\n",
        "                    for g in games]\n",
        "        return [(g.make_image(i), self._action_to_num(g.history[i:i + num_unroll_steps]),\n",
        "                g.make_target(i, num_unroll_steps, td_steps))\n",
        "                for (g, i) in game_pos]\n",
        "\n",
        "    # バッファーから均一または優先度に応じてサンプルゲーム。\n",
        "    def sample_game(self) -> Game:\n",
        "        n = random.randrange(len(self.buffer))\n",
        "        return self.buffer[n]\n",
        "\n",
        "    # ゲームから均一に、または優先度に応じて位置をサンプリングします。\n",
        "    def sample_position(self, game, num_unroll_steps: int, td_steps: int) -> int:\n",
        "        d = num_unroll_steps - td_steps\n",
        "        n = len(game.image) - (d if d > 0 else -d)\n",
        "        i = random.randrange(n)\n",
        "        return i\n",
        "\n",
        "    def _action_to_num(sellf, action_list: List[Action]) -> List:\n",
        "        return [a.index for a in action_list]"
      ],
      "metadata": {
        "id": "7JwBs-XclISm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RepresentationNetwork:\n",
        "    def __init__(self):\n",
        "        '''in:(None, 32, 6), out:(None, 8, 96)'''\n",
        "        self.obs_shape = (32, 6)\n",
        "        self.nn_actions = 3\n",
        "        self.filters = 48\n",
        "        self.arr = [[8, 5, 3, 1],[8, 5, 3, 1],[8, 5, 3]]\n",
        "        self.filter = [self.filters, self.filters * 2, self.filters * 2]\n",
        "\n",
        "        self.kr = l2(0.0005)\n",
        "        #self.opt = SGD(learning_rate = 0.001, momentum = 0.9)\n",
        "        self.opt = Adam(learning_rate=0.0001, epsilon=0.001)\n",
        "        self.units = 64\n",
        "\n",
        "        self._main_network_layer()\n",
        "\n",
        "    def _main_network_layer(self):\n",
        "        x = input = Input(shape = self.obs_shape) # (None, 32, 6)\n",
        "        for a, f in zip(self.arr, self.filter):\n",
        "            x = self._residual_layer(a, f)(x)\n",
        "\n",
        "        # x: (None, 32, 96)\n",
        "        x = AveragePooling1D(4, padding='same')(x) # (None, 8, 96)\n",
        "\n",
        "        a, b, c = tf.shape(x) # (None, 8, 96)\n",
        "        a = 1 if a is None else a\n",
        "\n",
        "        x_min = tf.fill([a, b, c], tf.reduce_min(x))\n",
        "        x_max = tf.fill([a, b, c], tf.reduce_max(x))\n",
        "\n",
        "        hidden_states = (x - x_min) / (x_max - x_min)\n",
        "\n",
        "        model = Model(inputs = input, outputs= hidden_states)\n",
        "        model.compile(loss = 'categorical_crossentropy', optimizer = self.opt, metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def _residual_layer(self, arr, filter):\n",
        "        def f(input_block):\n",
        "            x = input_block\n",
        "            for a in arr:\n",
        "                if a >=  5:\n",
        "                    x = self._conv_layer(filter, a, True)(x)\n",
        "                elif a == 3:\n",
        "                    x = self._conv_layer(filter, a, False)(x)\n",
        "                else:\n",
        "                    if len(arr) == 3:\n",
        "                        input_block = BatchNormalization()(input_block)\n",
        "                    else:\n",
        "                        input_block = self._conv_layer(filter,\n",
        "                                                       a, False)(input_block)\n",
        "\n",
        "            x = Add()([x, input_block])\n",
        "            x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "    def _conv_layer(self, filters, kernel_size  = 1, join_act = True):\n",
        "        def f(input_block):\n",
        "            x = Conv1D(filters=filters, kernel_size=kernel_size, \n",
        "                       padding=\"same\", use_bias=False, \n",
        "                       kernel_regularizer=l2(0.0005),\n",
        "                       kernel_initializer=\"he_normal\")(input_block)\n",
        "            x = BatchNormalization()(x)\n",
        "            if join_act:\n",
        "                x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "class PredictionNetwork:\n",
        "    '''in:(None, 8, 96), out:(None, 3), (None, 1)'''\n",
        "    def __init__(self):\n",
        "        self.hidden_state_shape = (8, 96)\n",
        "        self.nn_actions = 3\n",
        "        self.filters = 96\n",
        "        self.arr = [[8, 5, 3, 1],[8, 5, 3, 1],[8, 5, 3]]\n",
        "        self.filter = [self.filters, self.filters * 2, self.filters * 2]\n",
        "\n",
        "        self.kr = l2(0.0005)\n",
        "        #self.opt = SGD(learning_rate = 0.001, momentum = 0.9)\n",
        "        self.opt = Adam(learning_rate=0.0001, epsilon=0.001)\n",
        "\n",
        "        self._main_network_layer()\n",
        "\n",
        "    def _main_network_layer(self):\n",
        "        x = input = Input(shape = self.hidden_state_shape)\n",
        "\n",
        "        for a, f in zip(self.arr, self.filter):\n",
        "            x = self._residual_layer(a, f)(x)\n",
        "\n",
        "        # x: (None, 8, 192)\n",
        "        #x = AveragePooling1D(8, padding='same')(x) # (None, 1, 192)\n",
        "        x = GlobalAveragePooling1D()(x) # (None, 128)\n",
        "\n",
        "        p = Dense(self.nn_actions, kernel_regularizer=self.kr, activation='softmax')(x)\n",
        "        v = Dense(1, kernel_regularizer=self.kr, activation='tanh')(x)\n",
        "\n",
        "        model = Model(inputs = input, outputs= [p, v])\n",
        "        model.compile(loss = 'categorical_crossentropy', optimizer = self.opt, metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "    def _residual_layer(self, arr, filter):\n",
        "        def f(input_block):\n",
        "            x = input_block\n",
        "            for a in arr:\n",
        "                if a >=  5:\n",
        "                    x = self._conv_layer(filter, a, True)(x)\n",
        "                elif a == 3:\n",
        "                    x = self._conv_layer(filter, a, False)(x)\n",
        "                else:\n",
        "                    if len(arr) == 3:\n",
        "                        input_block = BatchNormalization()(input_block)\n",
        "                    else:\n",
        "                        input_block = self._conv_layer(filter,\n",
        "                                                       a, False)(input_block)\n",
        "\n",
        "            x = Add()([x, input_block])\n",
        "            x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "    def _conv_layer(self, filters, kernel_size  = 1, join_act = True):\n",
        "        def f(input_block):\n",
        "            x = Conv1D(filters=filters, kernel_size=kernel_size, \n",
        "                       padding=\"same\", use_bias=False, \n",
        "                       kernel_regularizer=l2(0.0005),\n",
        "                       kernel_initializer=\"he_normal\")(input_block)\n",
        "            x = BatchNormalization()(x)\n",
        "            if join_act:\n",
        "                x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "class DynamicsNetwork:\n",
        "    '''in:(None, 8, 96), (None, 1), out:(None, 8, 96), (None, 1)'''\n",
        "    def __init__(self):\n",
        "        self.hidden_state_shape = (8, 96)\n",
        "        self.action_shape = (1,)\n",
        "        self.nn_actions = 3\n",
        "        self.filters = 96\n",
        "        self.arr = [[8, 5, 3, 1],[8, 5, 3, 1],[8, 5, 3]]\n",
        "        self.filter = [self.filters, self.filters * 2, self.filters * 2]\n",
        "\n",
        "        self.kr = l2(0.0005)\n",
        "        #self.opt = SGD(learning_rate = 0.001, momentum = 0.9)\n",
        "        self.opt = Adam(learning_rate=0.0001, epsilon=0.001)\n",
        "\n",
        "        self._main_network_layer()\n",
        "\n",
        "    def _main_network_layer(self):\n",
        "        hs = input_hidden_state = Input(shape = self.hidden_state_shape)\n",
        "        ia = input_action = Input(shape = self.action_shape)\n",
        "\n",
        "        a, b, c = tf.shape(hs) # (None, 8, 96)\n",
        "        a = 1 if a is None else a\n",
        "        \n",
        "        actions_onehot = tf.transpose(tf.reshape(tf.repeat(\n",
        "            tf.one_hot(tf.cast(ia, dtype='int32'), self.nn_actions),\n",
        "            repeats = b, axis=1), (a, self.nn_actions, b)), perm=[0, 2, 1])\n",
        "        x = tf.concat([hs, actions_onehot], axis=2) #: (1, 8, 96 + 3)\n",
        "\n",
        "        for a, f in zip(self.arr, self.filter):\n",
        "            x = self._residual_layer(a, f)(x)\n",
        "\n",
        "        x = self._conv_layer(self.filters)(x)\n",
        "\n",
        "        a, b, c = tf.shape(hs) # (None, 8, 99)\n",
        "        a = 1 if a is None else a\n",
        "\n",
        "        x_min = tf.fill([a, b, c], tf.reduce_min(x))\n",
        "        x_max = tf.fill([a, b, c], tf.reduce_max(x))\n",
        "\n",
        "        hidden_states = (x - x_min) / (x_max - x_min)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(61, kernel_regularizer=l2(0.0005),\n",
        "                  kernel_initializer=\"he_normal\")(x)\n",
        "        categorical_rewards = tf.nn.softmax(x)\n",
        "\n",
        "        model = Model(inputs = [input_hidden_state, input_action],\n",
        "                      outputs= [hidden_states, categorical_rewards])\n",
        "        model.compile(loss = 'categorical_crossentropy', optimizer = self.opt,\n",
        "                      metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "    def _residual_layer(self, arr, filter):\n",
        "        def f(input_block):\n",
        "            x = input_block\n",
        "            for a in arr:\n",
        "                if a >=  5:\n",
        "                    x = self._conv_layer(filter, a, True)(x)\n",
        "                elif a == 3:\n",
        "                    x = self._conv_layer(filter, a, False)(x)\n",
        "                else:\n",
        "                    if len(arr) == 3:\n",
        "                        input_block = BatchNormalization()(input_block)\n",
        "                    else:\n",
        "                        input_block = self._conv_layer(filter,\n",
        "                                                       a, False)(input_block)\n",
        "\n",
        "            x = Add()([x, input_block])\n",
        "            x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "    def _conv_layer(self, filters, kernel_size  = 1, join_act = True):\n",
        "        def f(input_block):\n",
        "            x = Conv1D(filters=filters, kernel_size=kernel_size, \n",
        "                       padding=\"same\", use_bias=False, \n",
        "                       kernel_regularizer=l2(0.0005),\n",
        "                       kernel_initializer=\"he_normal\")(input_block)\n",
        "            x = BatchNormalization()(x)\n",
        "            if join_act:\n",
        "                x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f"
      ],
      "metadata": {
        "id": "DL56qBoTeFE5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkOutput(typing.NamedTuple):\n",
        "    '''\n",
        "    データ送信用クラス\n",
        "    typing.NamedTuple: 型がついているcollection.namedtupleを定義できる\n",
        "    a = Action(2)\n",
        "    net = NetworkOutput(4, 1.2, {a: 3.3}, [123.0, 555.6, 76.0])\n",
        "    print(net.value, net.reward, net.policy_logits[a], net.hidden_state)\n",
        "    (4, 1.2, 3.3, [123.0, 555.6, 76.0])\n",
        "    '''\n",
        "    value: float\n",
        "    reward: float\n",
        "    policy_logits: Dict[Action, float] # {Action(0): 0.1, Action(1): 0.5, Action(2): 0.4}\n",
        "    hidden_state: List[float]"
      ],
      "metadata": {
        "id": "mtnEybmCeE_0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, actor_num = 99):\n",
        "        self.rnet = RepresentationNetwork()\n",
        "        self.pnet = PredictionNetwork()\n",
        "        self.dnet = DynamicsNetwork()\n",
        "        self.rnet_model = self.rnet.model\n",
        "        self.pnet_model = self.pnet.model\n",
        "        self.dnet_model = self.dnet.model\n",
        "        self.rnet_name = 'muzero_RepresentationNetwork'\n",
        "        self.pnet_name = 'muzero_PredictionNetwork'\n",
        "        self.dnet_name = 'muzero_DynamicsNetwork'\n",
        "        self.training_steps_num = int(1)\n",
        "\n",
        "        self._load_network()\n",
        "        if actor_num == 0:\n",
        "            self.training_steps_num = int(10e5)\n",
        "        elif actor_num == 1:\n",
        "            self.training_steps_num = int(700e3)\n",
        "        elif actor_num == 2:\n",
        "            self.training_steps_num = int(550e3)\n",
        "\n",
        "    # representation + prediction function\n",
        "    def initial_inference(self, image) -> NetworkOutput:\n",
        "        '''何もしてないので、報酬報酬0に設定。\n",
        "        image: (32, 6), hidden_state: (1, 8, 96)\n",
        "        policy_logits: (1, 3), value: (1, 1)'''\n",
        "        hidden_state = self.rnet_model(tf.convert_to_tensor(image))\n",
        "        policy_logits, value = self.pnet_model(hidden_state)\n",
        "        dic = {}\n",
        "        for i, l in enumerate(policy_logits[0]):\n",
        "            dic[Action(i)] = l\n",
        "        return NetworkOutput(self._rescaling_inverse(value[0][0].numpy()), 0.0, dic, hidden_state)\n",
        "\n",
        "    # dynamics + prediction function\n",
        "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
        "        '''hidden_state: (1, 8, 96), action:(1, )\n",
        "        categorical_rewards: (1, 61)\n",
        "        hidden_state:  (1, 8, 96), reward:  (1, 61)'''\n",
        "        act = tf.convert_to_tensor([action.index])\n",
        "        hidden_state, categorical_rewards = self.dnet_model([hidden_state, act])\n",
        "        policy_logits, value = self.pnet_model(hidden_state)\n",
        "\n",
        "        supports = tf.range(-30, 31, dtype=tf.float32)\n",
        "        reward = tf.reduce_sum(supports * categorical_rewards, axis=1, keepdims=True)\n",
        "        dic = {}\n",
        "        for i, l in enumerate(policy_logits[0]):\n",
        "            dic[Action(i)] = l\n",
        "        return NetworkOutput(self._rescaling_inverse(value[0][0].numpy()), reward[0][0].numpy(), dic, hidden_state)\n",
        "\n",
        "    # このネットワークの重みを返します。\n",
        "    def get_weights(self):\n",
        "        return [self.rnet_model.get_weights(),\n",
        "                self.pnet_model.get_weights(),\n",
        "                self.dnet_model.get_weights()]\n",
        "\n",
        "    # ネットワークが訓練されたステップ/バッチの数。\n",
        "    def training_steps(self) -> int:\n",
        "        '''\n",
        "        training_steps < 500e3 -> 1.0\n",
        "        training_steps < 600e3 -> 0.75\n",
        "        training_steps < 750e3 -> 0.5\n",
        "        over                   -> 0.25\n",
        "        '''\n",
        "        return self.training_steps_num\n",
        "\n",
        "    def _rescaling_inverse(self, x):\n",
        "        eps = 0.001\n",
        "        if x > 0:\n",
        "            return ((2*eps*x+2*eps+1-\n",
        "                        (4*eps*(eps+1+x)+1)**0.5)/(2*eps**2))\n",
        "        else:\n",
        "            return ((-2*eps*x+2*eps+1-\n",
        "                        (4*eps*(eps+1-x)+1)**0.5)/(2*eps**2)*(-1))\n",
        "            \n",
        "    def save_network(self):\n",
        "        self.rnet_model.save_weights(f'{game_dir}/{self.rnet_name}.h5')\n",
        "        self.pnet_model.save_weights(f'{game_dir}/{self.pnet_name}.h5')\n",
        "        self.dnet_model.save_weights(f'{game_dir}/{self.dnet_name}.h5')\n",
        "\n",
        "    def _load_network(self):\n",
        "        if os.path.isfile(f'{game_dir}/{self.rnet_name}.h5'):\n",
        "            self.rnet_model.load_weights(f'{game_dir}/{self.rnet_name}.h5')\n",
        "        if os.path.isfile(f'{game_dir}/{self.pnet_name}.h5'):\n",
        "            self.pnet_model.load_weights(f'{game_dir}/{self.pnet_name}.h5')\n",
        "        if os.path.isfile(f'{game_dir}/{self.dnet_name}.h5'):\n",
        "            self.dnet_model.load_weights(f'{game_dir}/{self.dnet_name}.h5')"
      ],
      "metadata": {
        "id": "EhzI70k0eE5Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SharedStorage:\n",
        "    def __init__(self):\n",
        "        self._networks = {}\n",
        "\n",
        "    def latest_network(self, actor_num = 99) -> Network:\n",
        "        if self._networks:\n",
        "            return self._networks[max(self._networks.keys())]\n",
        "        else:\n",
        "            # policy -> uniform, value -> 0, reward -> 0\n",
        "            return self._make_uniform_network(actor_num)\n",
        "\n",
        "    def save_network(self, step: int, network: Network):\n",
        "        self._networks[step] = network\n",
        "\n",
        "    def _make_uniform_network(self, actor_num = 99):\n",
        "        return Network(actor_num)"
      ],
      "metadata": {
        "id": "vkWhiNBDoKN4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### Part 1: Self-Play ########\n",
        "# 各セルフプレイジョブは他のすべてのジョブとは独立しています。\n",
        "# 最新のネットワークスナップショットを取得し、ゲームを作成し、共有リプレイバッファに書き込むことでトレーニングジョブで利用できるようにします。\n",
        "def run_selfplay(config: MuZeroConfig, storage: SharedStorage, actor_num: int=99):\n",
        "\n",
        "    network = storage.latest_network(actor_num)\n",
        "    game = play_game(config, network)\n",
        "    print(game.info)\n",
        "    return game\n",
        "\n",
        "# 各ゲームは、最初のボードの位置から開始し、ゲームの終了に達するまで動きを生成するためにモンテカルロツリー検索を繰り返し実行することによって生成されます。\n",
        "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
        "\n",
        "    game = config.new_game()\n",
        "    i = 0\n",
        "\n",
        "    while not game.terminal():\n",
        "        # 検索ツリーのルートでは、表現関数を使用して、現在の観測値を指定して隠し状態を取得します。\n",
        "        if i >= 32:\n",
        "            root = Node(0)\n",
        "            current_observation = game.make_image(-1) # 最新のimageを取得\n",
        "            expand_node(root, game.legal_actions(),\n",
        "                        network.initial_inference(current_observation))\n",
        "            add_exploration_noise(config, root)\n",
        "            # 次に、アクションシーケンスとネットワークによって学習されたモデルのみを使用してモンテカルロツリー検索を実行します。\n",
        "            run_mcts(config, root, game.action_history(), network)\n",
        "            action = select_action(config, len(game.history), root, network)\n",
        "            game.apply(action)\n",
        "            game.store_search_statistics(root)\n",
        "        else:\n",
        "            game.apply(Action(1))\n",
        "        i += 1\n",
        "    return game\n",
        "\n",
        "# コアモンテカルロツリー検索アルゴリズム。\n",
        "# アクションを決定するために、Nシミュレーションを実行し、常に検索ツリーのルートから始まり、リーフノードに到達するまでUCB式に従ってツリーを横断します。\n",
        "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
        "             network: Network):\n",
        "    min_max_stats = MinMaxStats(config.known_bounds)\n",
        "\n",
        "    for _ in range(config.num_simulations):\n",
        "        history = action_history.clone()\n",
        "        node = root\n",
        "        search_path = [node]\n",
        "\n",
        "        while node.expanded():\n",
        "            action, node = select_child(config, node, min_max_stats)\n",
        "            history.add_action(action)\n",
        "            search_path.append(node)\n",
        "\n",
        "        # 検索ツリー内では、ダイナミクス関数を使用して、アクションと前の非表示状態を指定して次の非表示状態を取得します。\n",
        "        parent = search_path[-2]\n",
        "        network_output = network.recurrent_inference(parent.hidden_state,\n",
        "                                                    history.last_action())\n",
        "        expand_node(node, history.action_space(), network_output)\n",
        "        backpropagate(search_path, network_output.value, config.discount, min_max_stats)\n",
        "\n",
        "\n",
        "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
        "                  network: Network):\n",
        "    visit_counts = [\n",
        "        (child.visit_count, action) for action, child in node.children.items()\n",
        "    ]\n",
        "    t = config.visit_softmax_temperature_fn(\n",
        "        num_moves=num_moves, training_steps=network.training_steps())\n",
        "\n",
        "    _, action = softmax_sample(visit_counts, t)\n",
        "    return action\n",
        "\n",
        "def softmax_sample(distribution, temperature: float):\n",
        "    '''\n",
        "    [(30, <__main__.Action object at 0x7fd9fcd5b650>),\n",
        "     (20, <__main__.Action object at 0x7fd9fcd5b150>)]\n",
        "    '''\n",
        "    p, n = np.array([]), np.array([], dtype=int)\n",
        "    for i, ll in enumerate(distribution):\n",
        "        p = np.append(p, ll[0])\n",
        "        n = np.append(n, i)\n",
        "    f = np.exp(p/temperature)/np.sum(np.exp(p/temperature))\n",
        "    a = np.random.choice(a=n, p=f)\n",
        "    return 0, distribution[a][1]\n",
        "\n",
        "# UCBスコアが最も高い子を選択します。\n",
        "def select_child(config: MuZeroConfig, node: Node,\n",
        "                 min_max_stats: MinMaxStats):\n",
        "    _, action, child = max(\n",
        "        (ucb_score(config, node, child, min_max_stats), action,\n",
        "        child) for action, child in node.children.items())\n",
        "    return action, child\n",
        "\n",
        "\n",
        "# ノードのスコアは、その値と以前の探査ボーナスに基づいています。\n",
        "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\n",
        "              min_max_stats: MinMaxStats) -> float:\n",
        "    pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                    config.pb_c_base) + config.pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    prior_score = pb_c * child.prior\n",
        "    value_score = min_max_stats.normalize(child.value())\n",
        "    return prior_score + value_score\n",
        "\n",
        "\n",
        "# ニューラルネットワークから得られた値、報酬、ポリシー予測を使用してノードを拡張します。\n",
        "def expand_node(node: Node, actions: List[Action],\n",
        "                network_output: NetworkOutput):\n",
        "    node.hidden_state = network_output.hidden_state\n",
        "    node.reward = network_output.reward\n",
        "\n",
        "    policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
        "    policy_sum = sum(policy.values())\n",
        "    for action, p in policy.items():\n",
        "        node.children[action] = Node(p / policy_sum)\n",
        "\n",
        "\n",
        "# シミュレーションの最後に、ツリーからルートまで評価を伝播します。\n",
        "def backpropagate(search_path: List[Node], value: float,\n",
        "                  discount: float, min_max_stats: MinMaxStats):\n",
        "    for node in search_path:\n",
        "        node.value_sum += value\n",
        "        node.visit_count += 1\n",
        "        min_max_stats.update(node.value())\n",
        "\n",
        "        value = node.reward + discount * value\n",
        "\n",
        "# 各検索の開始時に、ルートの前にディリクレノイズを追加し、検索が新しいアクションを探索することを奨励します。\n",
        "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
        "    actions = list(node.children.keys())\n",
        "    noise = np.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
        "    frac = config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "######### End Self-Play ##########\n",
        "####### Part 2: Training #########\n",
        "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "    network = Network(True)\n",
        "    pb = Progbar(config.training_steps)\n",
        "\n",
        "    for i in range(config.training_steps):\n",
        "        if i % config.checkpoint_interval == 0:\n",
        "            storage.save_network(i, network)\n",
        "        batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
        "        update_weights(network, batch, config)\n",
        "        pb.add(1)\n",
        "    storage.save_network(config.training_steps, network)\n",
        "\n",
        "def update_weights(network: Network, batch, config: MuZeroConfig):\n",
        "    # Initial step, from the real observation.\n",
        "    '''\n",
        "    batch:(1024, 3)\n",
        "    target_value, target_reward, target_policy = target\n",
        "    images: (1024, 32, 6)\n",
        "    actions: (1024, 5)\n",
        "    target_value: (1024, 6)\n",
        "    target_reward: (1024, 6)\n",
        "    target_policy: (1024, 6, 3)\n",
        "    config.batch_size=1024\n",
        "    '''\n",
        "\n",
        "    images, actions, targets = list(zip(*batch))\n",
        "    images = [i[0] for i in images]\n",
        "    images = tf.convert_to_tensor(images) # (1024, 32, 6)\n",
        "    actions = tf.convert_to_tensor(actions) # (1024, 5)\n",
        "\n",
        "    target_values, target_rewards, target_policys = [],[],[]\n",
        "    for target in targets:\n",
        "        mini_value, mini_reward, mini_policy = [],[],[]\n",
        "        for value, reward, policy in target:\n",
        "            mini_value.append(value)\n",
        "            mini_reward.append(reward)\n",
        "            mini_policy.append(policy)\n",
        "        target_values.append(mini_value)\n",
        "        target_rewards.append(mini_reward)\n",
        "        target_policys.append(mini_policy)\n",
        "\n",
        "    target_values = tf.convert_to_tensor(target_values, dtype=tf.float32)\n",
        "    target_rewards = tf.convert_to_tensor(target_rewards, dtype=tf.float32)\n",
        "    target_policys = tf.convert_to_tensor(target_policys)\n",
        "\n",
        "    loss = 0.\n",
        "    policy_loss, value_loss, reward_loss = 0., 0., 0.\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        hidden_states = network.rnet_model(images, training=True)\n",
        "        policy_logits, values = network.pnet_model(hidden_states, training=True)\n",
        "\n",
        "        reward = tf.zeros([config.batch_size, 1],dtype=tf.dtypes.float32)\n",
        "\n",
        "        ploss = scalar_loss(policy_logits, target_policys[:,0,:])\n",
        "        vloss = scalar_loss(values, reshape_to_one(config, target_values[:,0]))\n",
        "        rloss = scalar_loss(reward, reshape_to_one(config, target_rewards[:,0]))\n",
        "\n",
        "        scale = 1.0\n",
        "\n",
        "        policy_loss += scale_gradient(ploss, scale)\n",
        "        value_loss += scale_gradient(vloss, scale)\n",
        "        reward_loss += scale_gradient(rloss, scale)\n",
        "\n",
        "        '''\n",
        "        hidden_states:  (1024, 8, 96)\n",
        "        policy_logits:  (1024, 3)\n",
        "        values:  (1024, 1)\n",
        "        actions[:, 0], shape=(1024,), dtype=int32)\n",
        "        categorical_rewards: (1024, 61)\n",
        "        reward: (1024, 1)\n",
        "        '''\n",
        "\n",
        "        scale = 1.0 / config.num_unroll_steps\n",
        "        for i in range(config.num_unroll_steps):\n",
        "            ''''5週しかしてないので、注意が必要'''\n",
        "            hidden_states, categorical_rewards = network.dnet_model([hidden_states,\n",
        "                                                                     reshape_to_one(config, actions[:, i])], training=True)\n",
        "            policy_logits, values = network.pnet_model(hidden_states, training=True)\n",
        "\n",
        "            reward = make_reward(categorical_rewards)\n",
        "\n",
        "            ploss = scalar_loss(policy_logits, target_policys[:,i+1,:])\n",
        "            vloss = scalar_loss(values, reshape_to_one(config, target_values[:,i+1]))\n",
        "            rloss = scalar_loss(reward, reshape_to_one(config, target_rewards[:,i+1]))\n",
        "\n",
        "            policy_loss += scale_gradient(ploss, scale)\n",
        "            value_loss += scale_gradient(vloss, scale)\n",
        "            reward_loss += scale_gradient(rloss, scale)\n",
        "\n",
        "            hidden_states = 0.5 * hidden_states + 0.5 * tf.stop_gradient(hidden_states) # (1024, 8, 96), dtype=float32)\n",
        "\n",
        "        policy_loss = tf.reduce_mean(policy_loss)\n",
        "        value_loss = tf.reduce_mean(value_loss)\n",
        "        reward_loss = tf.reduce_mean(reward_loss)\n",
        "\n",
        "        loss = policy_loss + value_loss + reward_loss\n",
        "\n",
        "    #: Gather trainable variables\n",
        "    models = [network.rnet_model, network.pnet_model]\n",
        "    variables = [m.trainable_variables for m in models]\n",
        "\n",
        "    grads = tape.gradient(loss, variables)\n",
        "    for v, g, m in zip(variables, grads, models):\n",
        "        tmp_grads, _ = tf.clip_by_global_norm(g, 40.0)\n",
        "        m.optimizer.apply_gradients(zip(tmp_grads, v))\n",
        "        #m.optimizer.apply_gradients((grad, var) for (grad, var) in \n",
        "        #                            zip(tmp_grads, v) if grad is not None)\n",
        "\n",
        "# ボードゲームのMSE、アタリのカテゴリ値間のクロスエントロピー。\n",
        "def scalar_loss(prediction, target):\n",
        "    #if np.prod(prediction.shape) == prediction.shape[0]: # (batch_size, 1)\n",
        "    #    return mean_squared_error(target, prediction)\n",
        "    return categorical_crossentropy(target, prediction)\n",
        "\n",
        "def scale_gradient(tensor: tf.Tensor, scale: float) -> tf.Tensor:\n",
        "    return tensor * scale + tf.stop_gradient(tensor) * (1 - scale)\n",
        "\n",
        "def reshape_to_one(config: MuZeroConfig, tensor: tf.Tensor):\n",
        "    return tf.reshape(tensor, (config.batch_size, 1)) \n",
        "\n",
        "def make_reward(categorical_rewards: tf.Tensor) -> tf.Tensor:\n",
        "    supports = tf.range(-30, 31, dtype=tf.float32)\n",
        "    reward = tf.reduce_sum(supports * categorical_rewards, axis=1, keepdims=True)\n",
        "    return reward\n",
        "\n",
        "def rescaling(x):\n",
        "    eps = 0.001\n",
        "    if x == 0:\n",
        "        return 0\n",
        "    n = math.sqrt(abs(x)+1) - 1\n",
        "    return (tf.math.sign(x)*n + eps*x)\n",
        "\n",
        "def rescaling_inverse(x):\n",
        "    eps = 0.001\n",
        "    if x > 0:\n",
        "        return ((2*eps*x+2*eps+1-\n",
        "                    (4*eps*(eps+1+x)+1)**0.5)/(2*eps**2))\n",
        "    else:\n",
        "        return ((-2*eps*x+2*eps+1-\n",
        "                    (4*eps*(eps+1-x)+1)**0.5)/(2*eps**2)*(-1))\n",
        "######### End Training ###########\n",
        "############################# End of pseudocode ################################"
      ],
      "metadata": {
        "id": "luXv3SPtjr6M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MuZeroトレーニングは、ネットワークトレーニングとセルフプレイデータ生成の2つの独立した部分に分かれています。\n",
        "# これら2つの部分は、最新のネットワークチェックポイントをトレーニングからセルフプレイに転送し、\n",
        "# 完成したゲームをセルフプレイからトレーニングに転送することによってのみ通信します。\n",
        "def muzero(config: MuZeroConfig):\n",
        "    storage = SharedStorage()\n",
        "    lock = multiprocessing.Lock()\n",
        "\n",
        "    worker = []\n",
        "    for i in range(config.num_actors):\n",
        "        '''config.num_actors: 4'''\n",
        "        p = Process(target=launch_job,\n",
        "                    args=(run_selfplay, config, storage, lock, i))\n",
        "        worker.append(p)\n",
        "        p.start()\n",
        "    for w in worker:\n",
        "        w.join()\n",
        "\n",
        "    file_name = game_dir + '/muzero_game.pkl'\n",
        "    replay_buffer = load(file_name)\n",
        "    train_network(config, storage, replay_buffer)\n",
        "\n",
        "    return storage.latest_network()\n",
        "\n",
        "def launch_job(f, config: MuZeroConfig, storage:SharedStorage, lock, actor_num: int):\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "    file_name = game_dir + '/muzero_game.pkl'\n",
        "    actor_num = 99\n",
        "    # actor_num = 0\n",
        "\n",
        "    # for i in range(5):\n",
        "    for i in range(2):\n",
        "        start_time = datetime.now()\n",
        "        game = f(config, storage, actor_num)\n",
        "        lock.acquire()\n",
        "        if os.path.isfile(file_name):\n",
        "            replay_buffer = load(file_name)\n",
        "            replay_buffer.save_game(game)\n",
        "        else:\n",
        "            replay_buffer = ReplayBuffer(config)\n",
        "            replay_buffer.save_game(game)\n",
        "        save(file_name, replay_buffer)\n",
        "        buffer_num = len(replay_buffer.buffer)\n",
        "        del replay_buffer\n",
        "        lock.release()\n",
        "        end_time = datetime.now() - start_time\n",
        "        print(f'actor_num: No.{str(actor_num + 1)} time: {end_time} game nums: {str(buffer_num)} pieces')\n",
        "\n",
        "#読み出し\n",
        "def load(file_name: str):\n",
        "    with open(file_name, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "#保存\n",
        "def save(file_name: str, replay_buffer: ReplayBuffer):\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pickle.dump(replay_buffer, f)"
      ],
      "metadata": {
        "id": "7WNtZTLojjbz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = muzero(make_trade_config())\n",
        "net.save_network()"
      ],
      "metadata": {
        "id": "aIRx8ErljwYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524ebc0c-e512-4bc8-95d6-c373a5a9a254"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cur_revenue': 1146346.1538085938, 'trade_time': 1, 'trade_win': 1}\n",
            "actor_num: No.100 time: 0:46:58.789688 game nums: 21 pieces\n",
            "{'cur_revenue': 1146346.1538085938, 'trade_time': 1, 'trade_win': 1}\n",
            "actor_num: No.100 time: 0:47:02.500794 game nums: 22 pieces\n",
            "{'cur_revenue': 1146346.1538085938, 'trade_time': 1, 'trade_win': 1}\n",
            "actor_num: No.100 time: 0:47:04.980500 game nums: 23 pieces\n",
            "{'cur_revenue': 1146346.1538085938, 'trade_time': 1, 'trade_win': 1}\n",
            "actor_num: No.100 time: 0:47:38.798756 game nums: 24 pieces\n",
            "{'cur_revenue': 1146346.1538085938, 'trade_time': 1, 'trade_win': 1}\n",
            "actor_num: No.100 time: 0:46:50.887554 game nums: 25 pieces\n",
            "{'cur_revenue': 1146346.1538085938, 'trade_time': 1, 'trade_win': 1}\n",
            "actor_num: No.100 time: 0:46:55.844641 game nums: 26 pieces\n",
            "{'cur_revenue': 1146346.1538085938, 'trade_time': 1, 'trade_win': 1}\n",
            "actor_num: No.100 time: 0:46:58.988049 game nums: 27 pieces\n",
            "{'cur_revenue': 1146346.1538085938, 'trade_time': 1, 'trade_win': 1}\n",
            "actor_num: No.100 time: 0:46:57.267117 game nums: 28 pieces\n",
            "1000/1000 [==============================] - 1599s 2s/step\n"
          ]
        }
      ]
    }
  ]
}