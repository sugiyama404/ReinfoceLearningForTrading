{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/MuZero/train_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOu66rFVeAVy",
        "outputId": "ff3f641f-2910-436e-fb5d-233ccb871878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "from google.colab import drive\n",
        "import copy\n",
        "import math\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from statistics import mean\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import collections\n",
        "import typing\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "from tensorflow.keras.models import Sequential, clone_model, Model\n",
        "from tensorflow.keras.layers import (Dense, ReLU, Input, Lambda, LSTM, Activation,\n",
        "                                     GlobalAveragePooling1D, Flatten,\n",
        "                                     MaxPool1D, Conv1D, Add, BatchNormalization, AveragePooling1D)\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import categorical_crossentropy, mean_squared_error\n",
        "from tensorflow.keras.utils import Progbar\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "mode = 'train'\n",
        "name = 'muzero'\n",
        "level = 1\n",
        "if level == 2:\n",
        "    name += name + 'lv2'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "game_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JpVLJCyn5Fnr"
      },
      "outputs": [],
      "source": [
        "####### Helpers ##########\n",
        "MAXIMUM_FLOAT_VALUE = float('inf')\n",
        "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
        "\n",
        "# ツリーの最小値を保持するクラス。\n",
        "class MinMaxStats:\n",
        "    def __init__(self, known_bounds: Optional[KnownBounds]):\n",
        "        self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        if self.maximum > self.minimum:\n",
        "        # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OnAPNXOW5FVF"
      },
      "outputs": [],
      "source": [
        "class MuZeroConfig:\n",
        "    def __init__(self,\n",
        "                action_space_size: int,\n",
        "                max_moves: int,\n",
        "                discount: float,\n",
        "                dirichlet_alpha: float,\n",
        "                num_simulations: int,\n",
        "                batch_size: int,\n",
        "                td_steps: int,\n",
        "                num_actors: int,\n",
        "                lr_init: float,\n",
        "                lr_decay_steps: float,\n",
        "                visit_softmax_temperature_fn,\n",
        "                known_bounds: Optional[KnownBounds] = None):\n",
        "        ### Self-Play\n",
        "        self.action_space_size = action_space_size\n",
        "        self.num_actors = num_actors\n",
        "\n",
        "        self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
        "        self.max_moves = max_moves\n",
        "        self.num_simulations = num_simulations\n",
        "        self.discount = discount\n",
        "\n",
        "        # ルート事前探査ノイズ。\n",
        "        self.root_dirichlet_alpha = dirichlet_alpha\n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB式\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "        # 環境で発生する値に関する情報がすでにある場合は、\n",
        "        # それらを使用して再スケーリングを初期化できます。\n",
        "        # これは厳密には必要ありませんが、ボードゲームでAlphaZeroと同じ動作を確立します。\n",
        "        self.known_bounds = known_bounds\n",
        "\n",
        "        ### Training\n",
        "        self.training_steps = int(10)\n",
        "        self.checkpoint_interval = int(2)\n",
        "        self.window_size = int(100)\n",
        "        self.batch_size = batch_size\n",
        "        self.num_unroll_steps = 5\n",
        "        self.td_steps = td_steps\n",
        "\n",
        "        self.weight_decay = 1e-4\n",
        "        self.momentum = 0.9\n",
        "\n",
        "        # 指数学習率のスケジュール\n",
        "        self.lr_init = lr_init\n",
        "        self.lr_decay_rate = 0.1\n",
        "        self.lr_decay_steps = lr_decay_steps\n",
        "\n",
        "        self.env = Environment(df, initial_money=1000000, mode='train')\n",
        "        self.scaler, self.scaler2 = self._standard_scaler(self.env)\n",
        "        \n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        rewards = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            rewards.append([reward])\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler2 = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        scaler2.fit(rewards)\n",
        "        return scaler, scaler2\n",
        "\n",
        "    def new_game(self):\n",
        "        return Game(self.action_space_size, self.discount,\n",
        "                    self.env, self.scaler, self.scaler2)\n",
        "\n",
        "def make_trade_config() -> MuZeroConfig:\n",
        "    def visit_softmax_temperature(num_moves, training_steps):\n",
        "        if training_steps < 500e3:\n",
        "            return 1.0\n",
        "        elif training_steps < 750e3:\n",
        "            return 0.5\n",
        "        else:\n",
        "            return 0.25\n",
        "\n",
        "    return MuZeroConfig(\n",
        "        action_space_size=3,\n",
        "        max_moves=10000,  # Half an hour at action repeat 4.\n",
        "        discount=0.997,\n",
        "        dirichlet_alpha=0.25,\n",
        "        num_simulations=50,\n",
        "        batch_size=1024,\n",
        "        td_steps=10,\n",
        "        num_actors=4,\n",
        "        lr_init=0.05,\n",
        "        lr_decay_steps=350e3,\n",
        "        visit_softmax_temperature_fn=visit_softmax_temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JcflUsnuJe3l"
      },
      "outputs": [],
      "source": [
        "class Action:\n",
        "    '''\n",
        "    a = Action(0)\n",
        "    b = Action(2)\n",
        "\n",
        "    mydict = {a: \"value for 0\", b: \"value for 2\"}\n",
        "    print(mydict[a], mydict[b]) # value for 0 value for 2\n",
        "    a.index = 2                     # →ハッシュ値が変わる\n",
        "    print(mydict[a], mydict[b]) # value for 2 value for 2\n",
        "    c = Action(3)\n",
        "    print(a in mydict)\n",
        "    print(c > a)\n",
        "    print(c == a)\n",
        "    '''\n",
        "    def __init__(self, index: int):\n",
        "        '''コンストラクタ'''\n",
        "        self.index = index\n",
        "\n",
        "    def __hash__(self):\n",
        "        '''hash呼び出し時に呼び出される'''\n",
        "        return self.index\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        '''同値の時に呼び出される'''\n",
        "        return self.index == other.index\n",
        "\n",
        "    def __gt__(self, other):\n",
        "        '''大小比較された時に呼び出される'''\n",
        "        return self.index > other.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "itiiZjCYTeud"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self) -> bool:\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self) -> float:\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ws2lFQnGKHuQ"
      },
      "outputs": [],
      "source": [
        "class ActionHistory:\n",
        "    '''検索内で使用されるシンプルな履歴コンテナ。\n",
        "    実行されたアクションを追跡するためにのみ使用されます。'''\n",
        "    def __init__(self, history: List[Action], action_space_size: int):\n",
        "        self.history = list(history)\n",
        "        self.action_space_size = action_space_size\n",
        "\n",
        "    def clone(self):\n",
        "        return ActionHistory(self.history, self.action_space_size)\n",
        "\n",
        "    def add_action(self, action: Action):\n",
        "        self.history.append(action)\n",
        "\n",
        "    def last_action(self) -> Action:\n",
        "        return self.history[-1]\n",
        "\n",
        "    def action_space(self) -> List[Action]:\n",
        "        return [Action(i) for i in range(self.action_space_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UesmX6O1olab"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test', commission = 0):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.commission      = commission\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "        self.sell_price      = None\n",
        "        self.buy_price       = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "        self.sell_price      = 0\n",
        "        self.buy_price       = 0\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self.sell_price = 0\n",
        "        self._trade(action,done)\n",
        "        reward = 0\n",
        "        if (self.sell_price > 0) and (self.buy_price > 0) and ((self.sell_price - self.buy_price) != 0):\n",
        "            reward = (self.sell_price - self.buy_price) / self.buy_price\n",
        "            self.buy_price = 0\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            if self.hold_a_position != 0:\n",
        "                self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                self.hold_a_position = 0\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.buy_price += self.now_price\n",
        "                            self.cash_in_hand -= self.now_price + self.commission * self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.sell_price += self.now_price * self.hold_a_position\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position - self.commission * self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    self.trade_time += 1\n",
        "                    if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                        self.trade_win += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wwV-6duMeHqL"
      },
      "outputs": [],
      "source": [
        "# 環境との相互作用の単一のエピソード。\n",
        "class Game:\n",
        "    def __init__(self, action_space_size: int,\n",
        "                 discount: float, env: Environment,\n",
        "                 scaler: StandardScaler, scaler2: StandardScaler):\n",
        "        self.env = env  # Game specific environment.\n",
        "        self.history = []\n",
        "        self.rewards = []\n",
        "        self.image = [] # (長さ, 32, 6) 出力用\n",
        "        self.image_histroy = [] # (長さ, 6) 一時保存保存用\n",
        "        self.child_visits = []\n",
        "        self.root_values = []\n",
        "        self.action_space_size = action_space_size\n",
        "        self.discount = discount\n",
        "        self.terminal_flag = False\n",
        "\n",
        "        self.scaler = scaler\n",
        "        self.scaler2 = scaler2\n",
        "\n",
        "        self.act_onehot = [[ 0, 0, 0],  # [0]: buy\n",
        "                           [ 0, 1, 0],  # [1]: hold\n",
        "                           [ 0, 0, 1]]  # [2]: sell\n",
        "        _ = self.env.reset()\n",
        "\n",
        "    # ゲーム固有の終了ルール。\n",
        "    def terminal(self) -> bool:\n",
        "        return self.terminal_flag\n",
        "\n",
        "    # 法的措置のゲーム固有の計算。\n",
        "    def legal_actions(self) -> List[Action]:\n",
        "        act_list = [Action(0), Action(1)]\n",
        "        if self.env.hold_a_position != 0:\n",
        "            act_list = [Action(1), Action(2)]\n",
        "        return act_list\n",
        "\n",
        "    # 環境を進める。\n",
        "    def apply(self, action: Action):\n",
        "        act = action.index\n",
        "        state, reward, done, info = self.env.step(act)\n",
        "        self.terminal_flag = done\n",
        "\n",
        "        if len(self.image_histroy) >= 31:\n",
        "            reward = self.scaler2.transform([[reward]])\n",
        "            reward = reward[0]\n",
        "            self.rewards.append(reward)\n",
        "            self.history.append(action)\n",
        "        self._make_image_histroy(state, act)\n",
        "\n",
        "    # image_histroyとimageの生成\n",
        "    def _make_image_histroy(self, state: list, act: int):\n",
        "        state = self.scaler.transform([state])\n",
        "        image = state[0].tolist() + self.act_onehot[act]\n",
        "        self.image_histroy.append(image)\n",
        "        if len(self.image_histroy) >= 32:\n",
        "            self.image.append(self.image_histroy[-32:])\n",
        "\n",
        "    def store_search_statistics(self, root: Node):\n",
        "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "        action_space = (Action(index) for index in range(self.action_space_size))\n",
        "        self.child_visits.append([\n",
        "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "            for a in action_space\n",
        "        ])\n",
        "        self.root_values.append(root.value())\n",
        "\n",
        "    # ゲーム固有の特徴平面(obs)\n",
        "    def make_image(self, state_index: int):\n",
        "        return [self.image[state_index]]\n",
        "\n",
        "    # 値ターゲットは、検索ツリーNステップの割引ルート値と、それまでのすべての報酬の割引合計です。\n",
        "    def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int):\n",
        "        '''\n",
        "        input: state_index: int, num_unroll_steps: 5, td_steps: 10)\n",
        "        output: target_value: TD目標価値(z), target_reward: 即時報酬(u), target_policy: MCTSポリシー(pai)\n",
        "        '''\n",
        "        targets = []\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "            bootstrap_index = current_index + td_steps\n",
        "            if bootstrap_index < len(self.root_values):\n",
        "                value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
        "                value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
        "\n",
        "            if current_index < len(self.root_values):\n",
        "                targets.append((value, self.rewards[current_index],\n",
        "                                self.child_visits[current_index]))\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states.\n",
        "                targets.append((0, 0, [0.333, 0.334, 0.333]))\n",
        "        return targets\n",
        "\n",
        "    def action_history(self) -> ActionHistory:\n",
        "        return ActionHistory(self.history, self.action_space_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7JwBs-XclISm"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, config: MuZeroConfig):\n",
        "        self.window_size = config.window_size\n",
        "        self.batch_size = config.batch_size\n",
        "        self.buffer = []\n",
        "\n",
        "    def save_game(self, game):\n",
        "        if len(self.buffer) > self.window_size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(game)\n",
        "\n",
        "    def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
        "        games = [self.sample_game() for _ in range(self.batch_size)]\n",
        "        game_pos = [(g, self.sample_position(g, num_unroll_steps, td_steps))\n",
        "                    for g in games]\n",
        "        return [(g.make_image(i), g.history[i:i + num_unroll_steps],\n",
        "                g.make_target(i, num_unroll_steps, td_steps))\n",
        "                for (g, i) in game_pos]\n",
        "\n",
        "    # バッファーから均一または優先度に応じてサンプルゲーム。\n",
        "    def sample_game(self) -> Game:\n",
        "        n = random.randrange(len(self.buffer))\n",
        "        return self.buffer[n]\n",
        "\n",
        "    # ゲームから均一に、または優先度に応じて位置をサンプリングします。\n",
        "    def sample_position(self, game, num_unroll_steps: int, td_steps: int) -> int:\n",
        "        d = num_unroll_steps - td_steps\n",
        "        n = len(game.image) - (d if d > 0 else -d)\n",
        "        i = random.randrange(n)\n",
        "        return i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DL56qBoTeFE5"
      },
      "outputs": [],
      "source": [
        "class RepresentationNetwork:\n",
        "    def __init__(self):\n",
        "        '''in:(None, 32, 6), out:(None, 8, 96)'''\n",
        "        self.obs_shape = (32, 6)\n",
        "        self.nn_actions = 3\n",
        "        self.filters = 48\n",
        "        self.arr = [[8, 5, 3, 1],[8, 5, 3, 1],[8, 5, 3]]\n",
        "        self.filter = [self.filters, self.filters * 2, self.filters * 2]\n",
        "\n",
        "        self.kr = l2(0.0005)\n",
        "        self.opt = SGD(learning_rate = 0.001, momentum = 0.9)\n",
        "        self.units = 64\n",
        "\n",
        "        self._main_network_layer()\n",
        "\n",
        "    def _main_network_layer(self):\n",
        "        x = input = Input(shape = self.obs_shape) # (None, 32, 6)\n",
        "        for a, f in zip(self.arr, self.filter):\n",
        "            x = self._residual_layer(a, f)(x)\n",
        "\n",
        "        # x: (None, 32, 96)\n",
        "        x = AveragePooling1D(4, padding='same')(x) # (None, 8, 96)\n",
        "\n",
        "        a, b, c = x.shape # (None, 8, 96)\n",
        "        a = 1 if a == None else a\n",
        "\n",
        "        x_min = tf.fill([a, b, c], tf.reduce_min(x))\n",
        "        x_max = tf.fill([a, b, c], tf.reduce_max(x))\n",
        "\n",
        "        hidden_states = (x - x_min) / (x_max - x_min)\n",
        "\n",
        "        model = Model(inputs = input, outputs= hidden_states)\n",
        "        model.compile(loss = 'mean_squared_error', optimizer = Adam(), metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def _residual_layer(self, arr, filter):\n",
        "        def f(input_block):\n",
        "            x = input_block\n",
        "            for a in arr:\n",
        "                if a >=  5:\n",
        "                    x = self._conv_layer(filter, a, True)(x)\n",
        "                elif a == 3:\n",
        "                    x = self._conv_layer(filter, a, False)(x)\n",
        "                else:\n",
        "                    if len(arr) == 3:\n",
        "                        input_block = BatchNormalization()(input_block)\n",
        "                    else:\n",
        "                        input_block = self._conv_layer(filter,\n",
        "                                                       a, False)(input_block)\n",
        "\n",
        "            x = Add()([x, input_block])\n",
        "            x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "    def _conv_layer(self, filters, kernel_size  = 1, join_act = True):\n",
        "        def f(input_block):\n",
        "            x = Conv1D(filters=filters\n",
        "                       , kernel_size=kernel_size,\n",
        "                       padding=\"same\")(input_block)\n",
        "            x = BatchNormalization()(x)\n",
        "            if join_act:\n",
        "                x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "class PredictionNetwork:\n",
        "    '''in:(None, 8, 96), out:(None, 3), (None, 1)'''\n",
        "    def __init__(self):\n",
        "        self.hidden_state_shape = (8, 96)\n",
        "        self.nn_actions = 3\n",
        "        self.filters = 96\n",
        "        self.arr = [[8, 5, 3, 1],[8, 5, 3, 1],[8, 5, 3]]\n",
        "        self.filter = [self.filters, self.filters * 2, self.filters * 2]\n",
        "\n",
        "        self.kr = l2(0.0005)\n",
        "        self.opt = SGD(learning_rate = 0.001, momentum = 0.9)\n",
        "\n",
        "        self._main_network_layer()\n",
        "\n",
        "    def _main_network_layer(self):\n",
        "        x = input = Input(shape = self.hidden_state_shape)\n",
        "\n",
        "        for a, f in zip(self.arr, self.filter):\n",
        "            x = self._residual_layer(a, f)(x)\n",
        "\n",
        "        # x: (None, 8, 192)\n",
        "        #x = AveragePooling1D(8, padding='same')(x) # (None, 1, 192)\n",
        "        x = GlobalAveragePooling1D()(x) # (None, 128)\n",
        "\n",
        "        p = Dense(self.nn_actions, kernel_regularizer=self.kr, activation='softmax')(x)\n",
        "        v = Dense(1, kernel_regularizer=self.kr, activation='tanh')(x)\n",
        "\n",
        "        model = Model(inputs = input, outputs= [p, v])\n",
        "        model.compile(loss = 'mean_squared_error', optimizer = Adam(), metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "    def _residual_layer(self, arr, filter):\n",
        "        def f(input_block):\n",
        "            x = input_block\n",
        "            for a in arr:\n",
        "                if a >=  5:\n",
        "                    x = self._conv_layer(filter, a, True)(x)\n",
        "                elif a == 3:\n",
        "                    x = self._conv_layer(filter, a, False)(x)\n",
        "                else:\n",
        "                    if len(arr) == 3:\n",
        "                        input_block = BatchNormalization()(input_block)\n",
        "                    else:\n",
        "                        input_block = self._conv_layer(filter,\n",
        "                                                       a, False)(input_block)\n",
        "\n",
        "            x = Add()([x, input_block])\n",
        "            x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "    def _conv_layer(self, filters, kernel_size  = 1, join_act = True):\n",
        "        def f(input_block):\n",
        "            x = Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                       padding=\"same\")(input_block)\n",
        "            x = BatchNormalization()(x)\n",
        "            if join_act:\n",
        "                x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "class DynamicsNetwork:\n",
        "    '''in:(None, 8, 96), (None, 1), out:(None, 8, 96), (None, 1)'''\n",
        "    def __init__(self):\n",
        "        self.hidden_state_shape = (8, 96)\n",
        "        self.action_shape = (1,)\n",
        "        self.nn_actions = 3\n",
        "        self.filters = 96\n",
        "        self.arr = [[8, 5, 3, 1],[8, 5, 3, 1],[8, 5, 3]]\n",
        "        self.filter = [self.filters, self.filters * 2, self.filters * 2]\n",
        "\n",
        "        self.kr = l2(0.0005)\n",
        "        self.opt = SGD(learning_rate = 0.001, momentum = 0.9)\n",
        "\n",
        "        self._main_network_layer()\n",
        "\n",
        "    def _main_network_layer(self):\n",
        "        hs = input_hidden_state = Input(shape = self.hidden_state_shape)\n",
        "        ia = input_action = Input(shape = self.action_shape)\n",
        "\n",
        "        a, b, c = hs.shape # (None, 8, 96)\n",
        "        a = 1 if a == None else a\n",
        "        \n",
        "        actions_onehot = tf.transpose(tf.reshape(tf.repeat(\n",
        "            tf.one_hot(tf.cast(ia, dtype='int32'), self.nn_actions),\n",
        "            repeats = b, axis=1), (a, self.nn_actions, b)), perm=[0, 2, 1])\n",
        "        x = tf.concat([hs, actions_onehot], axis=2) #: (1, 8, 96 + 3)\n",
        "\n",
        "        for a, f in zip(self.arr, self.filter):\n",
        "            x = self._residual_layer(a, f)(x)\n",
        "\n",
        "        x = self._conv_layer(self.filters)(x)\n",
        "\n",
        "\n",
        "        a, b, c = x.shape # (None, 8, 99)\n",
        "        a = 1 if a == None else a\n",
        "\n",
        "        x_min = tf.fill([a, b, c], tf.reduce_min(x))\n",
        "        x_max = tf.fill([a, b, c], tf.reduce_max(x))\n",
        "\n",
        "        hidden_states = (x - x_min) / (x_max - x_min)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(61, kernel_regularizer=l2(0.0005),\n",
        "                  kernel_initializer=\"he_normal\")(x)\n",
        "        categorical_rewards = tf.nn.softmax(x)\n",
        "\n",
        "        model = Model(inputs = [input_hidden_state, input_action],\n",
        "                      outputs= [hidden_states, categorical_rewards])\n",
        "        model.compile(loss = 'categorical_crossentropy', optimizer = Adam())\n",
        "        self.model = model\n",
        "\n",
        "    def _residual_layer(self, arr, filter):\n",
        "        def f(input_block):\n",
        "            x = input_block\n",
        "            for a in arr:\n",
        "                if a >=  5:\n",
        "                    x = self._conv_layer(filter, a, True)(x)\n",
        "                elif a == 3:\n",
        "                    x = self._conv_layer(filter, a, False)(x)\n",
        "                else:\n",
        "                    if len(arr) == 3:\n",
        "                        input_block = BatchNormalization()(input_block)\n",
        "                    else:\n",
        "                        input_block = self._conv_layer(filter,\n",
        "                                                       a, False)(input_block)\n",
        "\n",
        "            x = Add()([x, input_block])\n",
        "            x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "    def _conv_layer(self, filters, kernel_size  = 1, join_act = True):\n",
        "        def f(input_block):\n",
        "            x = Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                       padding=\"same\")(input_block)\n",
        "            x = BatchNormalization()(x)\n",
        "            if join_act:\n",
        "                x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mtnEybmCeE_0"
      },
      "outputs": [],
      "source": [
        "class NetworkOutput(typing.NamedTuple):\n",
        "    '''\n",
        "    データ送信用クラス\n",
        "    typing.NamedTuple: 型がついているcollection.namedtupleを定義できる\n",
        "    a = Action(2)\n",
        "    net = NetworkOutput(4, 1.2, {a: 3.3}, [123.0, 555.6, 76.0])\n",
        "    print(net.value, net.reward, net.policy_logits[a], net.hidden_state)\n",
        "    (4, 1.2, 3.3, [123.0, 555.6, 76.0])\n",
        "    '''\n",
        "    value: float\n",
        "    reward: float\n",
        "    policy_logits: Dict[Action, float] # {Action(0): 0.1, Action(1): 0.5, Action(2): 0.4}\n",
        "    hidden_state: List[float]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EhzI70k0eE5Q"
      },
      "outputs": [],
      "source": [
        "class Network:\n",
        "    def __init__(self, laod_model = False):\n",
        "        self.rnet = RepresentationNetwork()\n",
        "        self.pnet = PredictionNetwork()\n",
        "        self.dnet = DynamicsNetwork()\n",
        "        self.rnet_model = self.rnet.model\n",
        "        self.pnet_model = self.pnet.model\n",
        "        self.dnet_model = self.dnet.model\n",
        "        self.rnet_name = 'RepresentationNetwork'\n",
        "        self.pnet_name = 'PredictionNetwork'\n",
        "        self.dnet_name = 'DynamicsNetwork'\n",
        "        if laod_model:\n",
        "            self._load_network()\n",
        "\n",
        "    # representation + prediction function\n",
        "    def initial_inference(self, image) -> NetworkOutput:\n",
        "        '''何もしてないので、報酬報酬0に設定。\n",
        "        image: (32, 6), hidden_state: (1, 8, 96)\n",
        "        policy_logits: (1, 3), value: (1, 1)'''\n",
        "        hidden_state = self.rnet_model.predict(image)\n",
        "        policy_logits, value = self.pnet_model.predict(hidden_state)\n",
        "        dic = {}\n",
        "        for i, l in enumerate(policy_logits[0]):\n",
        "            dic[Action(i)] = l\n",
        "        return NetworkOutput(self._rescaling_inverse(value[0][0]), 0, dic, hidden_state)\n",
        "\n",
        "    # dynamics + prediction function\n",
        "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
        "        '''hidden_state: (1, 8, 96), action:(1, )\n",
        "        categorical_rewards: (1, 61)\n",
        "        hidden_state:  (1, 8, 96), reward:  (1, 61)'''\n",
        "        act = tf.constant([action.index])\n",
        "        hidden_state, categorical_rewards = self.dnet_model.predict([hidden_state, act])\n",
        "        policy_logits, value = self.pnet_model.predict(hidden_state)\n",
        "\n",
        "        supports = tf.range(-30, 31, dtype=tf.float32)\n",
        "        reward = tf.reduce_sum(supports * categorical_rewards, axis=1, keepdims=True)\n",
        "        dic = {}\n",
        "        for i, l in enumerate(policy_logits[0]):\n",
        "            dic[Action(i)] = l\n",
        "        return NetworkOutput(self._rescaling_inverse(value[0][0]), reward[0][0], dic, hidden_state)\n",
        "\n",
        "    # このネットワークの重みを返します。\n",
        "    def get_weights(self):\n",
        "        return [self.rnet_model.get_weights(),\n",
        "                self.pnet_model.get_weights(),\n",
        "                self.dnet_model.get_weights()]\n",
        "\n",
        "    # ネットワークが訓練されたステップ/バッチの数。\n",
        "    def training_steps(self) -> int:\n",
        "        return 0\n",
        "\n",
        "    def _rescaling_inverse(self, x):\n",
        "        eps = 0.001\n",
        "        if x > 0:\n",
        "            return ((2*eps*x+2*eps+1-\n",
        "                        (4*eps*(eps+1+x)+1)**0.5)/(2*eps**2))\n",
        "        else:\n",
        "            return ((-2*eps*x+2*eps+1-\n",
        "                        (4*eps*(eps+1-x)+1)**0.5)/(2*eps**2)*(-1))\n",
        "            \n",
        "    def save_network(self):\n",
        "        self.rnet_model.save_weights(f'{game_dir}/{self.rnet_name}.h5')\n",
        "        self.pnet_model.save_weights(f'{game_dir}/{self.pnet_name}.h5')\n",
        "        self.dnet_model.save_weights(f'{game_dir}/{self.dnet_name}.h5')\n",
        "\n",
        "    def _load_network(self):\n",
        "        self.rnet_model.load_weights(f'{game_dir}/{self.rnet_name}.h5')\n",
        "        self.pnet_model.load_weights(f'{game_dir}/{self.pnet_name}.h5')\n",
        "        self.dnet_model.load_weights(f'{game_dir}/{self.dnet_name}.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vkWhiNBDoKN4"
      },
      "outputs": [],
      "source": [
        "class SharedStorage:\n",
        "    def __init__(self):\n",
        "        self._networks = {}\n",
        "\n",
        "    def latest_network(self) -> Network:\n",
        "        if self._networks:\n",
        "            return self._networks[max(self._networks.keys())]\n",
        "        else:\n",
        "            # policy -> uniform, value -> 0, reward -> 0\n",
        "            return self._make_uniform_network()\n",
        "\n",
        "    def save_network(self, step: int, network: Network):\n",
        "        self._networks[step] = network\n",
        "\n",
        "    def _make_uniform_network(self):\n",
        "        return Network()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "luXv3SPtjr6M"
      },
      "outputs": [],
      "source": [
        "####### Part 1: Self-Play ########\n",
        "# 各セルフプレイジョブは他のすべてのジョブとは独立しています。ムを作成し、共有リプレイバッファに書き込むことでトレーニングジョブで利用できるようにします。\n",
        "def run_selfplay(config: MuZeroConfig, storage: SharedStorage, replay_buffer: ReplayBuffer):\n",
        "\n",
        "    for i in range(10):\n",
        "# 最新のネットワークスナップショットを取得し、ゲー\n",
        "        print(f'start game no.{str(i)}')\n",
        "        network = storage.latest_network()\n",
        "        game = play_game(config, network)\n",
        "        replay_buffer.save_game(game)\n",
        "\n",
        "# 各ゲームは、最初のボードの位置から開始し、ゲームの終了に達するまで動きを生成するためにモンテカルロツリー検索を繰り返し実行することによって生成されます。\n",
        "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
        "\n",
        "    game = config.new_game()\n",
        "    i = 0\n",
        "    while not game.terminal():\n",
        "        # 検索ツリーのルートでは、表現関数を使用して、現在の観測値を指定して隠し状態を取得します。\n",
        "        if i >= 32:\n",
        "            action = random.choice(game.legal_actions())\n",
        "            game.apply(action)\n",
        "        else:\n",
        "            game.apply(Action(1))\n",
        "        i += 1\n",
        "    return game\n",
        "\n",
        "####### Part 2: Training #########\n",
        "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "    network = Network(True)\n",
        "    pb = Progbar(config.training_steps)\n",
        "\n",
        "    for i in range(config.training_steps):\n",
        "        if i % config.checkpoint_interval == 0:\n",
        "            storage.save_network(i, network)\n",
        "        batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
        "        update_weights(network, batch, config)\n",
        "        pb.add(1)\n",
        "    storage.save_network(config.training_steps, network)\n",
        "\n",
        "def update_weights(network: Network, batch, config: MuZeroConfig):\n",
        "    # Initial step, from the real observation.\n",
        "    for image, actions, targets in batch:\n",
        "\n",
        "        loss = 0.\n",
        "        policy_loss, value_loss, reward_loss = 0., 0., 0.\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            hidden_state = network.rnet_model(np.array(image), training=True)\n",
        "            policy_logits, value = network.pnet_model(hidden_state, training=True)\n",
        "            \n",
        "            reward = tf.convert_to_tensor([0.0])\n",
        "            value = rescaling_inverse(value[0])\n",
        "            predictions = [(1.0, value, reward, policy_logits[0])]\n",
        "\n",
        "            # Recurrent steps, from action and previous hidden state.\n",
        "            for action in actions:\n",
        "                hidden_state = 0.5 * hidden_state + 0.5 * tf.stop_gradient(hidden_state) # shape=(1, 8, 96), dtype=float32)\n",
        "\n",
        "                act = tf.convert_to_tensor([action.index])\n",
        "                hidden_state, categorical_rewards = network.dnet_model([hidden_state, act], training=True)\n",
        "                policy_logits, value = network.pnet_model(hidden_state, training=True)\n",
        "\n",
        "                supports = tf.range(-30, 31, dtype=tf.float32)\n",
        "                reward = tf.reduce_sum(supports * categorical_rewards, axis=1, keepdims=True)\n",
        "                reward = reward[0]\n",
        "                value = rescaling_inverse(value[0])\n",
        "\n",
        "                predictions.append((1.0 / len(actions), value, reward, policy_logits[0]))\n",
        "\n",
        "            for prediction, target in zip(predictions, targets):\n",
        "                gradient_scale, value, reward, policy_logits = prediction\n",
        "                target_value, target_reward, target_policy = target\n",
        "\n",
        "                p_loss = scalar_loss(policy_logits, tf.convert_to_tensor(target_policy))\n",
        "                v_loss = scalar_loss(value, rescaling(target_value))\n",
        "\n",
        "                r_loss = scalar_loss(reward, tf.convert_to_tensor([target_reward]))\n",
        "\n",
        "                policy_loss += p_loss\n",
        "                value_loss  += v_loss\n",
        "                reward_loss += r_loss\n",
        "\n",
        "                # loss += policy_loss + value_loss + reward_loss\n",
        "                loss += scale_gradient((policy_loss + value_loss + reward_loss), gradient_scale)\n",
        "\n",
        "        #: Gather trainable variables\n",
        "        models = [network.rnet_model, network.pnet_model]\n",
        "        variables = [m.trainable_variables for m in models]\n",
        "\n",
        "        grads = tape.gradient(loss, variables)\n",
        "        for v, g, m in zip(variables, grads, models):\n",
        "            tmp_grads, _ = tf.clip_by_global_norm(g, 40.0)\n",
        "            #m.optimizer.apply_gradients(zip(tmp_grads, v))\n",
        "            m.optimizer.apply_gradients((grad, var) for (grad, var) in \n",
        "                                        zip(tmp_grads, v) if grad is not None)\n",
        "\n",
        "# ボードゲームのMSE、アタリのカテゴリ値間のクロスエントロピー。\n",
        "def scalar_loss(prediction, target):\n",
        "    if np.prod(prediction.shape) == prediction.shape[0]: # (batch_size, 1)\n",
        "        return mean_squared_error(target, prediction)\n",
        "    return categorical_crossentropy(target, prediction)\n",
        "\n",
        "def scale_gradient(tensor: tf.Tensor, scale: float) -> tf.Tensor:\n",
        "    return tensor * scale + tf.stop_gradient(tensor) * (1 - scale)\n",
        "\n",
        "def rescaling(x):\n",
        "    eps = 0.001\n",
        "    if x == 0:\n",
        "        return 0\n",
        "    n = math.sqrt(abs(x)+1) - 1\n",
        "    return (tf.math.sign(x)*n + eps*x)\n",
        "\n",
        "def rescaling_inverse(x):\n",
        "    eps = 0.001\n",
        "    if x > 0:\n",
        "        return ((2*eps*x+2*eps+1-\n",
        "                    (4*eps*(eps+1+x)+1)**0.5)/(2*eps**2))\n",
        "    else:\n",
        "        return ((-2*eps*x+2*eps+1-\n",
        "                    (4*eps*(eps+1-x)+1)**0.5)/(2*eps**2)*(-1))\n",
        "######### End Training ###########\n",
        "############################# End of pseudocode ################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7WNtZTLojjbz"
      },
      "outputs": [],
      "source": [
        "# MuZeroトレーニングは、ネットワークトレーニングとセルフプレイデータ生成の2つの独立した部分に分かれています。\n",
        "# これら2つの部分は、最新のネットワークチェックポイントをトレーニングからセルフプレイに転送し、\n",
        "# 完成したゲームをセルフプレイからトレーニングに転送することによってのみ通信します。\n",
        "def muzero(config: MuZeroConfig):\n",
        "    storage = SharedStorage()\n",
        "    replay_buffer = ReplayBuffer(config)\n",
        "    file_name = game_dir + '/muzero_game.pkl'\n",
        "    replay_buffer = load(file_name)\n",
        "\n",
        "    # run_selfplay(config, storage, replay_buffer)\n",
        "    train_network(config, storage, replay_buffer)\n",
        "\n",
        "    return storage.latest_network()\n",
        "\n",
        "#読み出し\n",
        "def load(file_name):\n",
        "    with open(file_name, \"rb\") as f:\n",
        "        return pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aIRx8ErljwYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bec4340-e85a-4e05-c753-45bf63e14acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 9453s 946s/step\n"
          ]
        }
      ],
      "source": [
        "net = muzero(make_trade_config())\n",
        "net.save_network()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjrsRdsdCSNwuN2Wqrc1Kd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}