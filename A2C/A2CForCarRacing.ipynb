{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2CForCarRacing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN8EXgm4b80wGGjY/83IF9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/A2C/A2CForCarRacing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8bCDqyTz9Wp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21995b50-8646-4db3-976c-7568ec2cbcae"
      },
      "source": [
        "!pip uninstall gym -y # gym 0.17.3 was broken at 2021/11/08\n",
        "!pip install gym gym[box2d] tensorflow-addons  > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.21.0\n",
            "Uninstalling gym-0.21.0:\n",
            "  Successfully uninstalled gym-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cck23Zda0D63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33928c14-c690-4d18-b9f6-e20f5943d362"
      },
      "source": [
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, ReLU, Input, Lambda, Conv2D, Flatten\n",
        "from tensorflow.keras.losses import Huber\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.optimizers import RectifiedAdam\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "import math\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "!apt update  > /dev/null 2>&1\n",
        "!apt install xvfb  > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay  > /dev/null 2>&1\n",
        "from pyvirtualdisplay import Display\n",
        "d = Display()\n",
        "d.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f8733961f90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs85f7vT_Ze8"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        obs_shape = (96,96,3)\n",
        "        nb_actions = 5\n",
        "        opt = RectifiedAdam(learning_rate=0.0001, epsilon=0.001)\n",
        "        input_ = inputs = Input(shape=obs_shape)\n",
        "        loss=Huber()\n",
        "        common = Conv2D(16, kernel_size=(8, 8), strides=(4, 4), activation='relu')(inputs)\n",
        "        common = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), activation='relu')(common)\n",
        "        common = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu')(common)\n",
        "        common = Flatten()(common)\n",
        "\n",
        "        common = Dense(512, activation='relu')(common)\n",
        "        actor_layer  = Dense(nb_actions, activation=\"softmax\")(common)\n",
        "        critic_layer = Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = Model(input_, [actor_layer, critic_layer])\n",
        "        model.compile(loss = loss, optimizer=opt)\n",
        "        model.summary()\n",
        "        Brain.model = model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lem_RNKr_4ch"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        ps, _ = self.model(np.array([state]))\n",
        "        act_p = ps[0].numpy()\n",
        "        act_p = act_p if (not (np.isnan(act_p).any())) else self._nan_to_zero_softmax(act_p)[0]\n",
        "        return np.random.choice(5, p=act_p)\n",
        "\n",
        "    def _nan_to_zero_softmax(self, x):\n",
        "        x[np.isnan(x)] = 0\n",
        "        return self._softmax(x)\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        if (x.ndim == 1):\n",
        "            x = x[None,:]\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dkKYPhDAFwZ"
      },
      "source": [
        "class Critic(Brain):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.997\n",
        "        self.beta  = 0.1\n",
        "\n",
        "    def valuenetwork(self, val):\n",
        "        states, next_states, actions = val['state'], val['next_state'], val['act']\n",
        "        rewards, dones = val['reward'], val['done']\n",
        "\n",
        "        onehot_actions = tf.one_hot(actions, 5)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = Brain.model(states, training=True)\n",
        "            _, next_v = Brain.model(next_states, training=True)\n",
        "\n",
        "            a_pi = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            a_pi = tf.clip_by_value(a_pi, 1e-10, 1.0)\n",
        "\n",
        "            q = rewards + (1 - dones) * self.gamma * next_v\n",
        "            advantage = q - v\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage, a_pi, v)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, Brain.model.trainable_variables)\n",
        "        Brain.model.optimizer.apply_gradients(zip(gradients, Brain.model.trainable_variables))\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,a_pi,v):\n",
        "\n",
        "        a = tf.math.log(a_pi) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "        sigma = tf.math.reduce_std(v)\n",
        "        sigma = tf.math.square(sigma)\n",
        "        entropy = self.beta*0.5*(tf.math.log(2 * math.pi * sigma) + 1)\n",
        "        return entropy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m781NcBNAcHG"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : np.ndarray = np.empty((0,96, 96, 3))\n",
        "    next_state : np.ndarray = np.empty((0,96, 96, 3))\n",
        "    action : np.ndarray = np.array([],int)\n",
        "    reward : np.ndarray = np.array([])\n",
        "    done : np.ndarray = np.array([])\n",
        "    batch_size : int = 32\n",
        "\n",
        "    def reset_experiences(self):\n",
        "        self.state = np.empty((0,96, 96, 3))\n",
        "        self.next_state = np.empty((0,96, 96, 3))\n",
        "        self.action = np.array([],int)\n",
        "        self.reward = np.array([])\n",
        "        self.done = np.array([])\n",
        "\n",
        "    def set_experiences(self, state, next_state, action, reward, done):\n",
        "        state = np.reshape(state, [1, 96, 96, 3])\n",
        "        self.state = np.append(self.state, state, axis=0)\n",
        "        next_state = np.reshape(next_state, [1, 96, 96, 3])\n",
        "        self.next_state = np.append(self.next_state, next_state, axis=0)\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        self.done = np.append(self.done, np.array(done))\n",
        "\n",
        "    def get_experiences(self):\n",
        "        mb_index = np.random.choice(len(self.action), self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.state[mb_index], self.next_state[mb_index],\n",
        "                 self.action[mb_index], self.reward[mb_index], self.done[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def isGetter(self):\n",
        "        return True if (len(self.action) > self.batch_size) else False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9N6dj4MA5Jy"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, experiences, episodes_times = 1000):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.experiences = experiences\n",
        "        self.episodes_times = episodes_times\n",
        "\n",
        "    def play_game(self):\n",
        "        for episode in range(self.episodes_times):\n",
        "\n",
        "            if (episode % 10 == 0):\n",
        "                metrics_names = ['score']\n",
        "                if (int(str(self.episodes_times)[:-1])*10 == episode):\n",
        "                    pb_i = Progbar(int(str(self.episodes_times)[-1]), stateful_metrics=metrics_names)\n",
        "                else:\n",
        "                    pb_i = Progbar(10, stateful_metrics=metrics_names)\n",
        "                score_mean = np.array([])\n",
        "\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "            self.experiences.reset_experiences()\n",
        "    \n",
        "            while not done:\n",
        "                self.env.render()              \n",
        "                action = self.actor.policynetwork(state)\n",
        "                tmp_action = self._action_clipping(action)\n",
        "                next_state, reward, done, info = self.env.step(tmp_action)\n",
        "                score+=reward\n",
        "\n",
        "                self.experiences.set_experiences(state, next_state, action, reward, done)\n",
        "                if self.experiences.isGetter():\n",
        "                    m_batch = self.experiences.get_experiences()\n",
        "                    self.critic.valuenetwork(m_batch)\n",
        "                    self.experiences.reset_experiences()\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            score_mean = np.append(score_mean, score)\n",
        "            values = [('score',np.mean(score_mean))]\n",
        "            pb_i.add(1, values=values)\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def _action_clipping(self, val):\n",
        "        actions = np.array([[ 0, 0, 0],  # [0]: straight\n",
        "                            [ 0, 1, 0],  # [1]: acceleration\n",
        "                            [ 0, 0, 1],  # [2]: decelerate\n",
        "                            [ 1, 0, 0],  # [3]: Turn right\n",
        "                            [-1, 0, 0]]) # [4]: Turn left\n",
        "        return actions[val]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDiUeMsMA4-8",
        "outputId": "2861cb12-e70f-4c74-d913-0464eab1f85f"
      },
      "source": [
        "episodes_times = 300\n",
        "batch_size = 32\n",
        "\n",
        "actor = Actor()\n",
        "critic = Critic()\n",
        "experiences = ExperiencesMemory(batch_size = batch_size)\n",
        "gym.logger.set_level(40)\n",
        "env = gym.make('CarRacing-v0')\n",
        "env.unwrapped.verbose = 0\n",
        "env = wrappers.Monitor(env, './', force=True, video_callable=(lambda ep: ep % 25 == 0))\n",
        "main = Main(env, actor, critic, experiences, episodes_times)\n",
        "main.play_game()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 96, 96, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 23, 23, 16)   3088        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 10, 10, 32)   8224        ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 8, 8, 32)     9248        ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 2048)         0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          1049088     ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 5)            2565        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            513         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,072,726\n",
            "Trainable params: 1,072,726\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "10/10 [==============================] - 228s 21s/step - score: -22.0920\n",
            "10/10 [==============================] - 230s 23s/step - score: -28.7208\n",
            "10/10 [==============================] - 189s 21s/step - score: -12.3187\n",
            "10/10 [==============================] - 234s 23s/step - score: -27.8195\n",
            "10/10 [==============================] - 192s 19s/step - score: -17.5294\n",
            "10/10 [==============================] - 209s 19s/step - score: -12.8791\n",
            "10/10 [==============================] - 213s 21s/step - score: -21.9438\n",
            "10/10 [==============================] - 193s 21s/step - score: -18.2429\n",
            "10/10 [==============================] - 171s 19s/step - score: -14.0949\n",
            "10/10 [==============================] - 189s 21s/step - score: -17.1003\n",
            "10/10 [==============================] - 188s 17s/step - score: -15.2665\n",
            "10/10 [==============================] - 173s 19s/step - score: -11.8455\n",
            "10/10 [==============================] - 186s 18s/step - score: -8.8656\n",
            "10/10 [==============================] - 174s 17s/step - score: -13.6959\n",
            "10/10 [==============================] - 230s 23s/step - score: -29.8620\n",
            "10/10 [==============================] - 168s 14s/step - score: -11.8900\n",
            "10/10 [==============================] - 210s 21s/step - score: -23.1783\n",
            "10/10 [==============================] - 208s 21s/step - score: -20.8772\n",
            "10/10 [==============================] - 172s 17s/step - score: -15.4838\n",
            "10/10 [==============================] - 208s 21s/step - score: -23.3306\n",
            "10/10 [==============================] - 244s 23s/step - score: -30.5970\n",
            "10/10 [==============================] - 151s 14s/step - score: -10.8304\n",
            "10/10 [==============================] - 168s 18s/step - score: -8.2667\n",
            "10/10 [==============================] - 134s 14s/step - score: -10.0769\n",
            "10/10 [==============================] - 172s 17s/step - score: -13.1654\n",
            "10/10 [==============================] - 185s 16s/step - score: -12.5237\n",
            "10/10 [==============================] - 170s 16s/step - score: -14.5808\n",
            "10/10 [==============================] - 204s 20s/step - score: -17.2854\n",
            "10/10 [==============================] - 169s 16s/step - score: -15.0807\n",
            "10/10 [==============================] - 223s 22s/step - score: -26.2329\n"
          ]
        }
      ]
    }
  ]
}